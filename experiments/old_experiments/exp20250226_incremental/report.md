# Check adding dialogue to existing graph on some known datasets

## Issues and goals
Compare this incremental method with 1 step generation from set of dialogues
1 step generation encounters problems on long dialogues and bigger size of set

## Hypotheses and steps
1. Check if this method is better that 1 step generation. Reason: Supposedly adding 1 dialogue
to existing graph is easier for LLM, than building graph in 1 step from the whole set
2. 1st graph is generated by 1 step method
3. Incremental method has 2 options:
- building whole graph with LLM
- building nodes with LLM and adding edges by embeddings

## Results

1. SCHEMA:RentalCars_3:

Early tries to add dialogue to 1 dialogue graph showed that (o3/o1)-mini cannot find common nodes
So switched to chatgpt-4o-latest
1st graph is generated by o3-mini
Then add dialogues one by one with chatgpt-4o-latest

Building whole graph with LLM from 2-dialogue graph and 3d dialogue fails: there are isolated nodes

Adding nodes to 1-dialogue graph with LLM completes with error: node wrongly combined
utterances 'Will you be picking it up in Long Beach?' and 'Are you planning to pick it up from London?'
which caused breaking the dialogue flow logics.

Adding nodes to 2-dialogues graph with LLM fails: losing nodes

2. MSR-E2E:

Adding nodes to upto 7-dialogues graph completed with a light error: one node wrongly combined
utterances 'Your reservation was confirmed, enjoy your ride with UberX!' and
'Thanks for using our service!', which caused wrong cycle from a node to itself.
Adding edges to 8 dialogues graph takes about 25 minutes on 1P100

Adding nodes to 8-dialogues graph with LLM fails: losing nodes
Adding edges to 9 dialogues graph takes about 100 minutes on 1P100

Building whole graph from 8-dialogues graph fails: there are isolated nodes

3. Frames:
upto 3-dialogues Graphs were generated with problem of wrongly combined nodes containing different locations
No more graphs were generated because dialogues are different and graph looks like rays coming out from one center

Ð¡onclusion
This incremental method with chatgpt-4o-latest showed more stable results than 1step generation with o1-mini
on MSR-E2E
But nevertherless incremetal approach fails on larger dialogues and bigger number of dialogues
Incremental approach finds more common nodes than 1step
Embedding based building of edges takes much time for larger graphs on P100
LLM-based building of edges works poorly
o1/o3 models don't have temperature parameter, so their results have stability issues


## Future plans

All things to be considered by future researchers, plans on next experiments and so on