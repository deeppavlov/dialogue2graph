{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Graph Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "simple_graph_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Create a dialogue graph for a {topic} conversation that follows these rules:\n",
    "\n",
    "1. Each assistant message (node) must be a clear question or statement that expects a specific type of response\n",
    "2. Each user message (edge) must directly answer or respond to the previous assistant message\n",
    "3. Include these basic flows:\n",
    "   - Main success path (completing the primary task)\n",
    "   - Early exit path (user decides not to proceed)\n",
    "   - Return path (user wants to modify earlier choice)\n",
    "\n",
    "Example of correct flow:\n",
    "Assistant: \"What type of coffee would you like?\"\n",
    "User: \"A latte please\"\n",
    "Assistant: \"Would you like that hot or iced?\"\n",
    "User: \"Hot please\"\n",
    "\n",
    "Example of incorrect flow:\n",
    "Assistant: \"What type of coffee would you like?\"\n",
    "User: \"No thank you\" (This response doesn't match the question)\n",
    "\n",
    "Format:\n",
    "{{\n",
    "    \"edges\": [\n",
    "        {{ \"source\": 1, \"target\": 2, \"utterances\": [\"I'd like a coffee\"] }},\n",
    "        {{ \"source\": 2, \"target\": 3, \"utterances\": [\"A latte please\"] }}\n",
    "    ],\n",
    "    \"nodes\": [\n",
    "        {{ \"id\": 1, \"label\": \"welcome\", \"is_start\": true, \"utterances\": [\"Welcome! How can I help?\"] }},\n",
    "        {{ \"id\": 2, \"label\": \"ask_drink\", \"is_start\": false, \"utterances\": [\"What would you like to drink?\"] }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Return ONLY the JSON without any additional text.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting test run ===\n",
      "\n",
      "=== Starting generation for topic: ordering pizza ===\n",
      "1. Generating prompt...\n",
      "2. Calling LLM for graph generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Response received from LLM\n",
      "   Raw response content: ```json\n",
      "{\n",
      "    \"edges\": [\n",
      "        {\n",
      "            \"source\": 1,\n",
      "            \"target\": 2,\n",
      "            \"utterances\": [\n",
      "                \"I'd like a Margherita pizza\",\n",
      "                \"I'd like a Pepperoni pi...\n",
      "3. Parsing JSON response...\n",
      "   Cleaned response: {\n",
      "    \"edges\": [\n",
      "        {\n",
      "            \"source\": 1,\n",
      "            \"target\": 2,\n",
      "            \"utterances\": [\n",
      "                \"I'd like a Margherita pizza\",\n",
      "                \"I'd like a Pepperoni pizza\",\n",
      "  ...\n",
      "   ✅ JSON parsed successfully\n",
      "   Found 7 nodes and 10 edges\n",
      "4. Validating graph...\n",
      "   ❌ Error during validation: are_triplets_valid() missing 1 required positional argument: 'topic'\n",
      "\n",
      "❌ Overall error: are_triplets_valid() missing 1 required positional argument: 'topic'\n",
      "\n",
      "=== Final Results ===\n",
      "Topic: ordering pizza\n",
      "Is valid: False\n",
      "Validation details: Error occurred: are_triplets_valid() missing 1 required positional argument: 'topic'\n"
     ]
    }
   ],
   "source": [
    "from chatsky_llm_autoconfig.metrics.llm_metrics import are_triplets_valid\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Инициализация моделей\n",
    "gen_model = ChatOpenAI(\n",
    "    model='o1-mini',\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "valid_model = ChatOpenAI(\n",
    "    model='gpt-4o',\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "def clean_json_response(response: str) -> str:\n",
    "    \"\"\"Очищает ответ от маркеров кода и лишних символов\"\"\"\n",
    "    # Убираем маркеры кода markdown\n",
    "    response = response.replace('```json', '').replace('```', '')\n",
    "    # Убираем лишние пробелы в начале и конце\n",
    "    response = response.strip()\n",
    "    return response\n",
    "\n",
    "def generate_and_validate_graph(topic: str):\n",
    "    \"\"\"\n",
    "    Генерирует граф диалога по теме и проверяет его валидность\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Starting generation for topic: {topic} ===\")\n",
    "    \n",
    "    try:\n",
    "        # Генерируем граф\n",
    "        print(\"1. Generating prompt...\")\n",
    "        prompt = simple_graph_prompt.format(topic=topic)\n",
    "        \n",
    "        print(\"2. Calling LLM for graph generation...\")\n",
    "        try:\n",
    "            response = gen_model.invoke(prompt)\n",
    "            print(\"   Response received from LLM\")\n",
    "            print(f\"   Raw response content: {response.content[:200]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error during LLM call: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "        print(\"3. Parsing JSON response...\")\n",
    "        try:\n",
    "            cleaned_response = clean_json_response(response.content)\n",
    "            print(f\"   Cleaned response: {cleaned_response[:200]}...\")\n",
    "            graph = json.loads(cleaned_response)\n",
    "            print(f\"   ✅ JSON parsed successfully\")\n",
    "            print(f\"   Found {len(graph.get('nodes', []))} nodes and {len(graph.get('edges', []))} edges\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"   ❌ JSON parsing failed: {str(e)}\")\n",
    "            print(f\"   Problematic content: {cleaned_response}\")\n",
    "            raise e\n",
    "\n",
    "        print(\"4. Validating graph...\")\n",
    "        try:\n",
    "            validation_result = are_triplets_valid(graph, valid_model)\n",
    "            print(f\"   Validation completed: {validation_result['value']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error during validation: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "        return {\n",
    "            \"graph\": graph,\n",
    "            \"is_valid\": validation_result[\"value\"],\n",
    "            \"validation_details\": validation_result.get(\"reason\", \"No details provided\")\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Overall error: {str(e)}\")\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"is_valid\": False,\n",
    "            \"validation_details\": f\"Error occurred: {str(e)}\"\n",
    "        }\n",
    "\n",
    "# Пример использования\n",
    "topic = \"ordering pizza\"\n",
    "print(\"\\n=== Starting test run ===\")\n",
    "result = generate_and_validate_graph(topic)\n",
    "\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(f\"Topic: {topic}\")\n",
    "print(f\"Is valid: {result['is_valid']}\")\n",
    "print(f\"Validation details: {result['validation_details']}\")\n",
    "if result.get(\"graph\"):\n",
    "    print(\"\\nGenerated graph:\")\n",
    "    print(json.dumps(result[\"graph\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run generation\n",
    "topics = [\n",
    "    \"medical appointment scheduling\",\n",
    "    \"food delivery service\",\n",
    "    \"fitness membership registration\",\n",
    "    \"apartment rental application\",\n",
    "    \"tech support assistance\",\n",
    "    \"travel package booking\",\n",
    "    \"insurance policy purchase\",\n",
    "    \"pet grooming service\",\n",
    "    \"moving service arrangement\",\n",
    "    \"home cleaning service\"\n",
    "]\n",
    "\n",
    "\n",
    "# generate_valid_dialogues(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatsky_llm_autoconfig.algorithms.dialogue_generation import DialogueSampler\n",
    "from chatsky_llm_autoconfig.graph import Graph\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize sampler\n",
    "sampler = DialogueSampler()\n",
    "\n",
    "# Get all JSON files in the directory\n",
    "dataset_dir = Path(\"generated_datasets\")\n",
    "json_files = list(dataset_dir.glob(\"*.json\"))\n",
    "\n",
    "for json_path in json_files:\n",
    "    print(f\"\\nProcessing file: {json_path.name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load JSON file\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Process each graph\n",
    "    for entry in data:\n",
    "        print(f\"\\nTopic: {entry['topic']}\")\n",
    "        graph = Graph(entry[\"graph\"])\n",
    "        \n",
    "        # Sample dialogues\n",
    "        dialogues = sampler.invoke(graph, 1, -1)\n",
    "        \n",
    "        # Print all dialogues\n",
    "        for i, dialogue in enumerate(dialogues, 1):\n",
    "            print(f\"\\nDialogue {i}:\")\n",
    "            for message in dialogue.messages:\n",
    "                print(f\"- {message}\")\n",
    "            \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing graphs from: graph_gen/graph_gen.json\n",
      "\n",
      "Validating graph with topic: library\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.386954 seconds\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result: {'value': True, 'description': 'All transitions are valid.'}\n",
      "✅ Valid dialogue generated for topic: library\n",
      "\n",
      "Saved 1 valid dialogues to: valid_complex_graphs/valid_dialogues_20241217_172116.json\n"
     ]
    }
   ],
   "source": [
    "from chatsky_llm_autoconfig.algorithms.dialogue_generation import DialogueSampler\n",
    "from chatsky_llm_autoconfig.graph import BaseGraph\n",
    "from chatsky_llm_autoconfig.metrics.llm_metrics import are_triplets_valid\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def check_existing_graphs_and_sample() -> None:\n",
    "   sampler = DialogueSampler()\n",
    "   \n",
    "   valid_model = ChatOpenAI(\n",
    "       model='gpt-4o',\n",
    "       api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "       base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "   )\n",
    "   \n",
    "   # Path to the directory containing generated datasets\n",
    "   datasets_dir = Path(\"graph_gen\")\n",
    "   \n",
    "   valid_results = []\n",
    "   \n",
    "   # Iterate through json files in the datasets directory\n",
    "   for file_path in datasets_dir.glob(\"*.json\"):\n",
    "       try:\n",
    "           # Load existing graph array\n",
    "           with open(file_path, 'r', encoding='utf-8') as f:\n",
    "               graphs_data = json.load(f)\n",
    "           \n",
    "           print(f\"\\nProcessing graphs from: {file_path}\")\n",
    "           \n",
    "           # Process each graph in the array\n",
    "           for graph_data in graphs_data:\n",
    "               print(f\"\\nValidating graph with topic: {graph_data['topic']}\")\n",
    "               \n",
    "               graph_obj: BaseGraph = Graph(graph_data['graph'])\n",
    "\n",
    "               # Validate triplets\n",
    "               validation_result = are_triplets_valid(graph_obj, valid_model)\n",
    "               print(f\"Validation result: {validation_result}\")\n",
    "               \n",
    "               \n",
    "               if validation_result[\"value\"]:\n",
    "                   sampled_dialogues = sampler.invoke(graph_obj, 1, -1)\n",
    "                   valid_results.append({\n",
    "                       \"graph\": graph_data['graph'],\n",
    "                       \"topic\": graph_data['topic'],\n",
    "                       \"dialogues\": [d.model_dump() for d in sampled_dialogues],\n",
    "                       \"validation_result\": validation_result\n",
    "                   })\n",
    "                   print(f\"✅ Valid dialogue generated for topic: {graph_data['topic']}\")\n",
    "               else:\n",
    "                   print(f\"❌ Invalid dialogue for topic: {graph_data['topic']}\")\n",
    "               \n",
    "       except Exception as e:\n",
    "           print(f\"Error processing {file_path}: {str(e)}\")\n",
    "           continue\n",
    "\n",
    "   # Save results\n",
    "   if valid_results:\n",
    "       output_dir = Path(\"valid_complex_graphs\")\n",
    "       output_dir.mkdir(exist_ok=True)\n",
    "       \n",
    "       output_file = output_dir / f\"valid_dialogues_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "       with open(output_file, 'w', encoding='utf-8') as f:\n",
    "           json.dump(valid_results, f, ensure_ascii=False, indent=2)\n",
    "       print(f\"\\nSaved {len(valid_results)} valid dialogues to: {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   check_existing_graphs_and_sample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
