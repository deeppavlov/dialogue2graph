dialog2graph.metrics.no_llm_metrics.keys2graph.pipeline
=======================================================

.. py:module:: dialog2graph.metrics.no_llm_metrics.keys2graph.pipeline


Classes
-------

.. autoapisummary::

   dialog2graph.metrics.no_llm_metrics.keys2graph.pipeline.Pipeline


Functions
---------

.. autoapisummary::

   dialog2graph.metrics.no_llm_metrics.keys2graph.pipeline.compute_dialog_metrics
   dialog2graph.metrics.no_llm_metrics.keys2graph.pipeline.construct_graph_from_annotation
   dialog2graph.metrics.no_llm_metrics.keys2graph.pipeline.update_annotation_with_metrics


Module Contents
---------------

.. py:function:: compute_dialog_metrics(dialog_graphs)

   Вычисляет метрики для списка диалоговых графов:
   - max_dialog_depth: максимальное число переходов (ребер) в самом длинном пути
   - max_branching_factor: максимальное число исходящих переходов из одного узла
   - max_dialog_length: максимальное число узлов (ходов) в самом длинном пути

   :param dialog_graphs: список графов, каждый граф – словарь с ключом "graph",
                       содержащим "nodes" и "edges"
   :return: словарь с тремя ключами: 'max_dialog_depth', 'max_branching_factor', 'max_dialog_length'


.. py:function:: construct_graph_from_annotation(annotation)

   На основе аннотации строит упрощённое представление графа диалога.
   Предполагается, что диалог линейный и узлы берутся из mandatory_nodes,
   а для каждого узла берутся шаблоны из bot_utterances_templates.


.. py:function:: update_annotation_with_metrics(input_dict)

   Функция принимает на вход словарь с ключом "annotation". Если значения для
   max_dialog_depth, max_branching_factor или max_dialog_length равны "unknown",
   они вычисляются на основе структуры диалога (mandatory_nodes) и обновляются.

   :param input_dict: исходный словарь с аннотацией
   :return: обновлённый словарь с заполненными метриками, если они были неизвестны


.. py:class:: Pipeline

   Full pipeline class.
   Stores loaded data (self._all_data), selected subset (self._selected_data),
   and provides methods for annotation, generation, comparison, etc.


   .. py:method:: load_dialog_graphs(file_path: str) -> List[Dict[str, Any]]

      Load a list of items from a JSON file, store in self._all_data, return them.



   .. py:method:: load_graphs_for_comparison(original_file: str, generated_file: str) -> None

      Загружает списки графов из двух JSON-файлов:
       - original_file -> self._original_graphs
       - generated_file -> self._generated_graphs



   .. py:method:: add_graphs_to_test(selection: Union[str, List[int]], output_file: str) -> List[Dict[str, Any]]

      Choose a subset of loaded data by index or 'all',
      store in self._selected_data, return them.



   .. py:method:: annotate_graphs(model_name: str, temperature: float, output_file: str, graph_type: str, supports_user_role: bool = True) -> None

      Annotate only items that have either 'original_graph' or 'graph'.
      If none found, skip the item (silently).
      Save *only* annotated items to output_file.



   .. py:method:: generate_graphs_by_keys(keys: Union[List[str], str], model_name: str, temperature: float, output_file: str) -> None

      Generate new graphs from the annotation keys. Only applies to items
      that actually have item["annotation"] with a valid annotation dict.
      Save only changed items to output_file.



   .. py:method:: generate_graphs_from_dicts(list_of_key_dicts: List[Dict[str, Any]], model_name: str, temperature: float, output_file: str) -> None

      Create entirely new items in self._all_data for each dict in list_of_key_dicts,
      each with "generated_graph" as result.
      Save those newly created items to output_file.



   .. py:method:: estimate_costs_for_selected_graphs(model_name: str, approximate_completion_ratio: float = 1.0) -> None

      Roughly estimate token usage for annotation of selected items.
      Just sums tokens for system+user messages with the original_graph/graph.



   .. py:method:: compare_generated_graphs(output_metrics_file: str, threshold: float = SEMANTIC_THRESHOLD) -> None

      Compare original_graph vs generated_graph using:
       - direct metrics (metrics.triplet_match_accuracy)
       - semantic metrics (metrics_semantic.triplet_match_accuracy_semantic, compare_two_graphs_semantically)

      Only on items that have both original_graph (or graph) and generated_graph.
      Save results in output_metrics_file.



   .. py:method:: compare_annotations_of_original_and_generated(output_file: str) -> None

      Compare annotation["annotation"] of original vs generated_graph_annotation["annotation"].
      Use metrics_annotation.compare_annotation_differences to measure numeric diffs and semantic sim.
      Save results array to output_file.



   .. py:method:: compare_jaccard_similarity(model_name: str = 'o1-mini', output_file: str = 'jacard_metrics.json')

      Сравнивает self._original_graphs и self._generated_graphs
      по Semantic‑Jaccard метрикам (узлы и рёбра).

      Пишет детальный JSON в `output_file` и
      выводит средние значения в stdout.



   .. py:method:: calculate_graphs_similarity(output_file: str) -> None


