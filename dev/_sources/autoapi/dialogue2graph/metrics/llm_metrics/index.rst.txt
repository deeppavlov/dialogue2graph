dialogue2graph.metrics.llm_metrics
==================================

.. py:module:: dialogue2graph.metrics.llm_metrics


Submodules
----------

.. toctree::
   :maxdepth: 1

   /autoapi/dialogue2graph/metrics/llm_metrics/metrics/index
   /autoapi/dialogue2graph/metrics/llm_metrics/prompts/index


Classes
-------

.. autoapisummary::

   dialogue2graph.metrics.llm_metrics.GraphValidationResult


Functions
---------

.. autoapisummary::

   dialogue2graph.metrics.llm_metrics.are_triplets_valid
   dialogue2graph.metrics.llm_metrics.is_theme_valid
   dialogue2graph.metrics.llm_metrics.compare_graphs


Package Contents
----------------

.. py:function:: are_triplets_valid(G: dialogue2graph.pipelines.core.graph.Graph, model: langchain_core.language_models.chat_models.BaseChatModel, return_type: str = 'dict') -> Union[dict, GraphValidationResult]

   Validate dialogue graph structure and logical transitions between nodes.

   :param G: The dialogue graph to validate
   :type G: BaseGraph
   :param model: The LLM model to use for validation
   :type model: BaseChatModel
   :param return_type: Type of return value - either "dict" or "detailed"
   :type return_type: str

   :returns:     If return_type == "dict": {'value': bool, 'description': str}
                 If return_type == "detailed": GraphValidationResult
   :rtype: Union[dict, GraphValidationResult]


.. py:function:: is_theme_valid(G: dialogue2graph.pipelines.core.graph.BaseGraph, model: langchain_core.language_models.chat_models.BaseChatModel, topic: str) -> dict[str]

   Validate if the dialog stays on theme/topic throughout the conversation.

   :param G: The dialog graph to validate
   :type G: BaseGraph
   :param model: The LLM model to use for validation
   :type model: BaseChatModel
   :param topic: The expected topic of the dialog
   :type topic: str

   :returns: {'value': bool, 'description': str}
   :rtype: dict


.. py:function:: compare_graphs(G1: dialogue2graph.pipelines.core.graph.BaseGraph, G2: dialogue2graph.pipelines.core.graph.BaseGraph, embedder: str = 'BAAI/bge-m3', sim_th: float = 0.93, llm_comparer: str = 'o3-mini', formatter: str = 'gpt-3.5-turbo', device='cuda:0') -> dialogue2graph.pipelines.core.schemas.CompareResponse

   Compare two graphs via utterance embeddings similarity. If similarity is lower than `sim_th` value LLM llm_comparer is used for additional comparison.
   LLM formatter is used to keep LLM answer in a required format.

   :param G1: graphs to compare
   :param G2: graphs to compare
   :param embedder: name of embedding model on HuggingFace
   :param sim_th: similarity threshold to use LLM
   :param llm_comparer: name of LLM to compare graphs
   :param formatter: name of LLm modle to format llm_comparer output

   :returns: dict with True or False value and a description.


.. py:class:: GraphValidationResult

   Bases: :py:obj:`TypedDict`


   dict() -> new empty dictionary
   dict(mapping) -> new dictionary initialized from a mapping object's
       (key, value) pairs
   dict(iterable) -> new dictionary initialized as if via:
       d = {}
       for k, v in iterable:
           d[k] = v
   dict(**kwargs) -> new dictionary initialized with the name=value pairs
       in the keyword argument list.  For example:  dict(one=1, two=2)


   .. py:attribute:: is_valid
      :type:  bool


   .. py:attribute:: invalid_transitions
      :type:  List[InvalidTransition]


