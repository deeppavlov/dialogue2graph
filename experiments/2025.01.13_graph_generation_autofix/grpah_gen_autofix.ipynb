{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from chatsky_llm_autoconfig.algorithms.topic_graph_generation import CycleGraphGenerator\n",
    "from chatsky_llm_autoconfig.graph import BaseGraph\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import Optional, Dict, Any\n",
    "import networkx as nx\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_graph_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Create a dialogue graph for a {topic} conversation that will be used for training data generation. The graph must follow these requirements:\n",
    "\n",
    "1. Dialogue Flow Requirements:\n",
    "   - Each assistant message (node) must be a precise question or statement that expects a specific type of response\n",
    "   - Each user message (edge) must logically and directly respond to the previous assistant message\n",
    "   - All paths must maintain clear context and natural conversation flow\n",
    "   - Avoid any ambiguous or overly generic responses\n",
    "\n",
    "2. Graph Structure Requirements:\n",
    "   - Must contain at least 2 distinct cycles (return paths)\n",
    "   - Each cycle should allow users to:\n",
    "     * Return to previous choices for modification\n",
    "     * Restart specific parts of the conversation\n",
    "     * Change their mind about earlier decisions\n",
    "   - Include clear exit points from each major decision path\n",
    "   \n",
    "3. Core Path Types:\n",
    "   - Main success path (completing the intended task)\n",
    "   - Multiple modification paths (returning to change choices)\n",
    "   - Early exit paths (user decides to stop)\n",
    "   - Alternative success paths (achieving goal differently)\n",
    "\n",
    "Example of a good cycle structure:\n",
    "Assistant: \"What size coffee would you like?\"\n",
    "User: \"Medium please\"\n",
    "Assistant: \"Would you like that hot or iced?\"\n",
    "User: \"Actually, can I change my size?\"\n",
    "Assistant: \"Of course! What size would you like instead?\"\n",
    "\n",
    "Format:\n",
    "{{\n",
    "    \"edges\": [\n",
    "        {{\n",
    "            \"source\": \"node_id\",\n",
    "            \"target\": \"node_id\",\n",
    "            \"utterances\": [\"User response text\"]\n",
    "        }}\n",
    "    ],\n",
    "    \"nodes\": [\n",
    "        {{\n",
    "            \"id\": \"node_id\",\n",
    "            \"label\": \"semantic_label\",\n",
    "            \"is_start\": boolean,\n",
    "            \"utterances\": [\"Assistant message text\"]\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Requirements for node IDs:\n",
    "- Must be unique integers\n",
    "- Start node should have ID 1\n",
    "- IDs should increment sequentially\n",
    "\n",
    "Return ONLY the valid JSON without any additional text or explanations.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dialogue_graph(topic: str, gen_model: BaseChatModel, prompt: PromptTemplate) -> BaseGraph:\n",
    "    \"\"\"\n",
    "    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥—Ä–∞—Ñ –¥–∏–∞–ª–æ–≥–∞ –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–π —Ç–µ–º–µ\n",
    "    \n",
    "    Args:\n",
    "        topic: —Ç–µ–º–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∏–∞–ª–æ–≥–∞\n",
    "        gen_model: –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "    \n",
    "    Returns:\n",
    "        nx.DiGraph: —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≥—Ä–∞—Ñ\n",
    "    \"\"\"\n",
    "    print(f\"\\nüí´ Generating graph for topic: {topic}\")\n",
    "    \n",
    "    try:\n",
    "        graph_generator = CycleGraphGenerator(prompt=prompt)\n",
    "        graph = graph_generator.invoke(topic=topic, model=gen_model)\n",
    "        print(f\"‚úì Graph generated: {len(graph.graph_dict['nodes'])} nodes, {len(graph.graph_dict['edges'])} edges\")\n",
    "        \n",
    "        return graph\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Generation error: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_graph_requirements(\n",
    "    graph: BaseGraph,\n",
    "    min_cycles: int = 2\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≥—Ä–∞—Ñ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º\n",
    "    \n",
    "    Args:\n",
    "        graph: BaseGraph –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
    "        min_cycles: –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ç—Ä–µ–±—É–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ü–∏–∫–ª–æ–≤\n",
    "        \n",
    "    Returns:\n",
    "        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏:\n",
    "        {\n",
    "            \"meets_requirements\": bool,\n",
    "            \"cycles\": List[List[int]],\n",
    "            \"cycles_count\": int\n",
    "        }\n",
    "    \"\"\"\n",
    "    print(\"\\nüîç Checking graph requirements...\")\n",
    "    \n",
    "    try:\n",
    "        cycles = list(nx.simple_cycles(graph.graph))\n",
    "        cycles_count = len(cycles)\n",
    "        \n",
    "        print(f\"üîÑ Found {cycles_count} cycles in the graph:\")\n",
    "        for i, cycle in enumerate(cycles, 1):\n",
    "            print(f\"Cycle {i}: {' -> '.join(map(str, cycle + [cycle[0]]))}\")\n",
    "            \n",
    "        meets_requirements = cycles_count >= min_cycles\n",
    "        \n",
    "        if not meets_requirements:\n",
    "            print(f\"‚ùå Graph doesn't meet cycle requirements (minimum {min_cycles} cycles needed)\")\n",
    "        else:\n",
    "            print(\"‚úÖ Graph meets cycle requirements\")\n",
    "            \n",
    "        return {\n",
    "            \"meets_requirements\": meets_requirements,\n",
    "            \"cycles\": cycles,\n",
    "            \"cycles_count\": cycles_count\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Validation error: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí´ Generating graph for topic: ordering pizza\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Graph generated: 8 nodes, 18 edges\n",
      "\n",
      "üîç Checking graph requirements...\n",
      "üîÑ Found 6 cycles in the graph:\n",
      "Cycle 1: 1 -> 2 -> 3 -> 4 -> 5 -> 6 -> 1\n",
      "Cycle 2: 1 -> 2 -> 3 -> 4 -> 6 -> 1\n",
      "Cycle 3: 1 -> 2 -> 1\n",
      "Cycle 4: 2 -> 3 -> 2\n",
      "Cycle 5: 3 -> 4 -> 3\n",
      "Cycle 6: 4 -> 5 -> 4\n",
      "‚úÖ Graph meets cycle requirements\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from chatsky_llm_autoconfig.algorithms.dialogue_generation import DialogueSampler\n",
    "from chatsky_llm_autoconfig.metrics.automatic_metrics import all_utterances_present\n",
    "\n",
    "CYCLE_REQUIREMENT = 2\n",
    "\n",
    "gen_model = ChatOpenAI(\n",
    "        model='o1-mini',\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "        temperature=1\n",
    "    )\n",
    "    \n",
    "    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
    "graph = generate_dialogue_graph(\"ordering pizza\", gen_model, enhanced_graph_prompt)\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π\n",
    "validation_result = validate_graph_requirements(graph, min_cycles=CYCLE_REQUIREMENT)\n",
    "\n",
    "dial_sampler = DialogueSampler()\n",
    "sampled_dialogues = dial_sampler.invoke(graph, 1, -1)\n",
    "\n",
    "res = all_utterances_present(graph, sampled_dialogues)\n",
    "\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
