# Check adding dialog to existing graph on some known datasets

## Issues and goals
Compare this incremental method with 1 step generation from set of dialogs
1 step generation encounters problems on long dialogs and bigger size of set

## Hypotheses and steps
1. Check if this method is better that 1 step generation. Reason: Supposedly adding 1 dialog
to existing graph is easier for LLM, than building graph in 1 step from the whole set
2. 1st graph is generated by 1 step method
3. Incremental method has 2 options:
- building whole graph with LLM
- building nodes with LLM and adding edges by embeddings

## Results

1. SCHEMA:RentalCars_3:

Early tries to add dialog to 1 dialog graph showed that (o3/o1)-mini cannot find common nodes
So switched to chatgpt-4o-latest
1st graph is generated by o3-mini
Then add dialogs one by one with chatgpt-4o-latest

Building whole graph with LLM from 2-dialog graph and 3d dialog fails: there are isolated nodes

Adding nodes to 1-dialog graph with LLM completes with error: node wrongly combined
utterances 'Will you be picking it up in Long Beach?' and 'Are you planning to pick it up from London?'
which caused breaking the dialog flow logics.

Adding nodes to 2-dialogs graph with LLM fails: losing nodes

2. MSR-E2E:

Adding nodes to upto 7-dialogs graph completed with a light error: one node wrongly combined
utterances 'Your reservation was confirmed, enjoy your ride with UberX!' and
'Thanks for using our service!', which caused wrong cycle from a node to itself.
Adding edges to 8 dialogs graph takes about 25 minutes on 1P100

Adding nodes to 8-dialogs graph with LLM fails: losing nodes
Adding edges to 9 dialogs graph takes about 100 minutes on 1P100

Building whole graph from 8-dialogs graph fails: there are isolated nodes

3. Frames:
upto 3-dialogs Graphs were generated with problem of wrongly combined nodes containing different locations
No more graphs were generated because dialogs are different and graph looks like rays coming out from one center

Ð¡onclusion
This incremental method with chatgpt-4o-latest showed more stable results than 1step generation with o1-mini
on MSR-E2E
But nevertherless incremetal approach fails on larger dialogs and bigger number of dialogs
Incremental approach finds more common nodes than 1step
Embedding based building of edges takes much time for larger graphs on P100
LLM-based building of edges works poorly
o1/o3 models don't have temperature parameter, so their results have stability issues


## Future plans

All things to be considered by future researchers, plans on next experiments and so on