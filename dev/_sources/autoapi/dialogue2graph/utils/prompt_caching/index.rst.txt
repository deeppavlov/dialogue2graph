dialogue2graph.utils.prompt_caching
===================================

.. py:module:: dialogue2graph.utils.prompt_caching

.. autoapi-nested-parse::

   Prompt Caching
   --------------

   The module contains auxilary functions for caching LLM answers.



Attributes
----------

.. autoapisummary::

   dialogue2graph.utils.prompt_caching.logger


Functions
---------

.. autoapisummary::

   dialogue2graph.utils.prompt_caching.add_uuid_to_prompt
   dialogue2graph.utils.prompt_caching.setup_cache


Module Contents
---------------

.. py:data:: logger

.. py:function:: add_uuid_to_prompt(prompt: str, seed: int = None) -> str

   Add a UUID to the beginning of a prompt.


.. py:function:: setup_cache(use_in_memory: bool = False)

   Set up the LLM cache.

   :param use_in_memory: If True, use InMemoryCache instead of SQLAlchemyCache

   :returns: The configured cache object or None if setup fails


