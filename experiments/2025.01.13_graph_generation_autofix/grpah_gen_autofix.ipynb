{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhanced_graph_prompt = PromptTemplate.from_template(\n",
    "#     \"\"\"\n",
    "# Create a dialogue graph for a {topic} conversation that will be used for training data generation. The graph must follow these requirements:\n",
    "\n",
    "# 1. Dialogue Flow Requirements:\n",
    "#    - Each assistant message (node) must be a precise question or statement that expects a specific type of response\n",
    "#    - Each user message (edge) must logically and directly respond to the previous assistant message\n",
    "#    - All paths must maintain clear context and natural conversation flow\n",
    "#    - Avoid any ambiguous or overly generic responses\n",
    "\n",
    "# 2. Graph Structure Requirements:\n",
    "#    - Must contain at least 2 distinct cycles (return paths)\n",
    "#    - Each cycle should allow users to:\n",
    "#      * Return to previous choices for modification\n",
    "#      * Restart specific parts of the conversation\n",
    "#      * Change their mind about earlier decisions\n",
    "#    - Include clear exit points from each major decision path\n",
    "   \n",
    "# 3. Core Path Types:\n",
    "#    - Main success path (completing the intended task)\n",
    "#    - Multiple modification paths (returning to change choices)\n",
    "#    - Early exit paths (user decides to stop)\n",
    "#    - Alternative success paths (achieving goal differently)\n",
    "\n",
    "# Example of a good cycle structure:\n",
    "# Assistant: \"What size coffee would you like?\"\n",
    "# User: \"Medium please\"\n",
    "# Assistant: \"Would you like that hot or iced?\"\n",
    "# User: \"Actually, can I change my size?\"\n",
    "# Assistant: \"Of course! What size would you like instead?\"\n",
    "\n",
    "# Format:\n",
    "# {{\n",
    "#     \"edges\": [\n",
    "#         {{\n",
    "#             \"source\": \"node_id\",\n",
    "#             \"target\": \"node_id\",\n",
    "#             \"utterances\": [\"User response text\"]\n",
    "#         }}\n",
    "#     ],\n",
    "#     \"nodes\": [\n",
    "#         {{\n",
    "#             \"id\": \"node_id\",\n",
    "#             \"label\": \"semantic_label\",\n",
    "#             \"is_start\": boolean,\n",
    "#             \"utterances\": [\"Assistant message text\"]\n",
    "#         }}\n",
    "#     ]\n",
    "# }}\n",
    "\n",
    "# Requirements for node IDs:\n",
    "# - Must be unique integers\n",
    "# - Start node should have ID 1\n",
    "# - IDs should increment sequentially\n",
    "\n",
    "# Return ONLY the valid JSON without any additional text or explanations.\n",
    "# \"\"\"\n",
    "# )\n",
    "\n",
    "# graph_generator = CycleGraphGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def validate_graph_cycle_requirement(\n",
    "#     graph: BaseGraph,\n",
    "#     min_cycles: int = 2\n",
    "# ) -> Dict[str, Any]:\n",
    "#     \"\"\"\n",
    "#     –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≥—Ä–∞—Ñ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º\n",
    "    \n",
    "#     Args:\n",
    "#         graph: BaseGraph –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
    "#         min_cycles: –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ç—Ä–µ–±—É–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ü–∏–∫–ª–æ–≤\n",
    "        \n",
    "#     Returns:\n",
    "#         Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏:\n",
    "#         {\n",
    "#             \"meets_requirements\": bool,\n",
    "#             \"cycles\": List[List[int]],\n",
    "#             \"cycles_count\": int\n",
    "#         }\n",
    "#     \"\"\"\n",
    "#     print(\"\\nüîç Checking graph requirements...\")\n",
    "    \n",
    "#     try:\n",
    "#         cycles = list(nx.simple_cycles(graph.graph))\n",
    "#         cycles_count = len(cycles)\n",
    "        \n",
    "#         print(f\"üîÑ Found {cycles_count} cycles in the graph:\")\n",
    "#         for i, cycle in enumerate(cycles, 1):\n",
    "#             print(f\"Cycle {i}: {' -> '.join(map(str, cycle + [cycle[0]]))}\")\n",
    "            \n",
    "#         meets_requirements = cycles_count >= min_cycles\n",
    "        \n",
    "#         if not meets_requirements:\n",
    "#             print(f\"‚ùå Graph doesn't meet cycle requirements (minimum {min_cycles} cycles needed)\")\n",
    "#         else:\n",
    "#             print(\"‚úÖ Graph meets cycle requirements\")\n",
    "            \n",
    "#         return {\n",
    "#             \"meets_requirements\": meets_requirements,\n",
    "#             \"cycles\": cycles,\n",
    "#             \"cycles_count\": cycles_count\n",
    "#         }\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Validation error: {str(e)}\")\n",
    "#         raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from chatsky_llm_autoconfig.metrics.llm_metrics import graph_validation\n",
    "# import json\n",
    "# repair_template = PromptTemplate.from_template(\"\"\"\n",
    "# Fix the invalid transitions in this dialogue graph while keeping its structure.\n",
    "\n",
    "# Current invalid transitions that need to be fixed:\n",
    "# {invalid_transitions}\n",
    "\n",
    "# Original graph structure:\n",
    "# {graph_json}\n",
    "\n",
    "# Requirements for the fix:\n",
    "# 1. Keep all node IDs and structure the same\n",
    "# 2. Fix ONLY the invalid transitions\n",
    "# 3. Make sure the fixed transitions are logical and natural\n",
    "# 4. Each user response must logically follow from the assistant's previous message\n",
    "# 5. Each assistant response must properly address the user's input\n",
    "\n",
    "# Return ONLY the complete fixed graph JSON with the same structure.\n",
    "# \"\"\")\n",
    "\n",
    "# def check_and_fix_transitions(graph: BaseGraph, graph_generator: CycleGraphGenerator, model: BaseChatModel, max_attempts: int = 3) -> Dict[str, Any]:\n",
    "#     \"\"\"\n",
    "#     –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–µ—Ä–µ—Ö–æ–¥—ã –≤ –≥—Ä–∞—Ñ–µ –∏ –ø—ã—Ç–∞–µ—Ç—Å—è –∏—Å–ø—Ä–∞–≤–∏—Ç—å –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ —á–µ—Ä–µ–∑ LLM\n",
    "    \n",
    "#     Args:\n",
    "#         graph: –ò—Å—Ö–æ–¥–Ω—ã–π –≥—Ä–∞—Ñ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è\n",
    "#         graph_generator: –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≥—Ä–∞—Ñ–æ–≤ –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è\n",
    "#         model: –ú–æ–¥–µ–ª—å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "#         max_attempts: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è\n",
    "        \n",
    "#     Returns:\n",
    "#         Dict: {\n",
    "#             \"is_valid\": bool,  # –£–¥–∞–ª–æ—Å—å –ª–∏ –ø–æ–ª—É—á–∏—Ç—å –≤–∞–ª–∏–¥–Ω—ã–π –≥—Ä–∞—Ñ\n",
    "#             \"graph\": BaseGraph,  # –ü–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è –≥—Ä–∞—Ñ–∞ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –∏–ª–∏ –Ω–µ—Ç)\n",
    "#             \"validation_details\": {  # –î–µ—Ç–∞–ª–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "#                 \"invalid_transitions\": [...],  # –°–ø–∏—Å–æ–∫ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö –ø–µ—Ä–µ—Ö–æ–¥–æ–≤\n",
    "#                 \"attempts_made\": int,  # –°–∫–æ–ª—å–∫–æ –ø–æ–ø—ã—Ç–æ–∫ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –±—ã–ª–æ —Å–¥–µ–ª–∞–Ω–æ\n",
    "#                 \"fixed_count\": int,  # –°–∫–æ–ª—å–∫–æ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ —É–¥–∞–ª–æ—Å—å –∏—Å–ø—Ä–∞–≤–∏—Ç—å\n",
    "#             }\n",
    "#         }\n",
    "#     \"\"\"\n",
    "#     # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–Ω–∞—á–∞–ª—å–Ω—ã–π –≥—Ä–∞—Ñ\n",
    "#     initial_validation = graph_validation(graph, model)\n",
    "#     if initial_validation[\"is_valid\"]:\n",
    "#         return {\n",
    "#             \"is_valid\": True,\n",
    "#             \"graph\": graph,\n",
    "#             \"validation_details\": {\n",
    "#                 \"invalid_transitions\": [],\n",
    "#                 \"attempts_made\": 0,\n",
    "#                 \"fixed_count\": 0\n",
    "#             }\n",
    "#         }\n",
    "    \n",
    "#     initial_invalid_count = len(initial_validation[\"invalid_transitions\"])\n",
    "#     current_graph = graph\n",
    "#     current_attempt = 0\n",
    "    \n",
    "#     while current_attempt < max_attempts:\n",
    "#         print(f\"\\nüîÑ Fix attempt {current_attempt + 1}/{max_attempts}\")\n",
    "        \n",
    "           \n",
    "#         try:\n",
    "#             # –ò—Å–ø–æ–ª—å–∑—É–µ–º graph_generator –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∞\n",
    "#             current_graph = graph_generator.invoke(model=model, prompt=repair_template, invalid_transitions=initial_validation[\"invalid_transitions\"], graph_json=current_graph.graph_dict)\n",
    "            \n",
    "#             # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –≥—Ä–∞—Ñ\n",
    "#             validation = graph_validation(current_graph, model)\n",
    "#             if validation[\"is_valid\"]:\n",
    "#                 return {\n",
    "#                     \"is_valid\": True,\n",
    "#                     \"graph\": current_graph,\n",
    "#                     \"validation_details\": {\n",
    "#                         \"invalid_transitions\": [],\n",
    "#                         \"attempts_made\": current_attempt + 1,\n",
    "#                         \"fixed_count\": initial_invalid_count\n",
    "#                     }\n",
    "#                 }\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"‚ö†Ô∏è Error during fix attempt: {str(e)}\")\n",
    "#             break\n",
    "            \n",
    "#         current_attempt += 1\n",
    "    \n",
    "#     remaining_invalid = len(validation[\"invalid_transitions\"])\n",
    "    \n",
    "#     return {\n",
    "#         \"is_valid\": False,\n",
    "#         \"graph\": current_graph,\n",
    "#         \"validation_details\": {\n",
    "#             \"invalid_transitions\": validation[\"invalid_transitions\"],\n",
    "#             \"attempts_made\": current_attempt,\n",
    "#             \"fixed_count\": initial_invalid_count - remaining_invalid\n",
    "#         }\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from chatsky_llm_autoconfig.algorithms.dialogue_generation import  RecursiveDialogueSampler\n",
    "# from chatsky_llm_autoconfig.metrics.automatic_metrics import all_utterances_present\n",
    "# from chatsky_llm_autoconfig.metrics.llm_metrics import graph_validation, is_theme_valid\n",
    "\n",
    "# CYCLE_REQUIREMENT = 2\n",
    "\n",
    "#  # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
    "# gen_model = ChatOpenAI(\n",
    "#     model='o1-mini',\n",
    "#     api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "#     base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "#     temperature=1\n",
    "# )\n",
    "\n",
    "# try:\n",
    "#     topic = \"ordering pizza\"\n",
    "        \n",
    "#     graph = graph_generator.invoke(model=gen_model, prompt=enhanced_graph_prompt, topic=topic)\n",
    "\n",
    "#     # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π\n",
    "#     validation_result = validate_graph_cycle_requirement(graph, min_cycles=CYCLE_REQUIREMENT)\n",
    "\n",
    "#     # –°–µ–º–ø–ª–∏–Ω–≥ –¥–∏–∞–ª–æ–≥–æ–≤\n",
    "#     dial_sampler = RecursiveDialogueSampler()\n",
    "#     sampled_dialogues = dial_sampler.invoke(graph, 1, -1)\n",
    "\n",
    "#     # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ–º–ø–ª–∏–Ω–≥–∞\n",
    "#     sampling_result = all_utterances_present(graph, sampled_dialogues)\n",
    "    \n",
    "#     if sampling_result is False:\n",
    "#         raise ValueError(\"Failed to sample valid dialogues from the graph or sampling error occurred\")\n",
    "        \n",
    "#     # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å —Ç–µ–º—ã\n",
    "#     theme_validation = is_theme_valid(graph, gen_model, topic)\n",
    "#     if not theme_validation['value']:\n",
    "#         raise ValueError(f\"Theme validation failed: {theme_validation['description']}\")\n",
    "    \n",
    "#     # –ï—Å–ª–∏ —Ç–µ–º–∞ –≤–∞–ª–∏–¥–Ω–∞, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç—Ä–∏–ø–ª–µ—Ç—ã –≤ —Ü–∏–∫–ª–µ\n",
    "#     transition_validation = check_and_fix_transitions(graph, graph_generator, gen_model, max_attempts=3)\n",
    "    \n",
    "#     print(\"\\nValidation results:\")\n",
    "#     print(f\"Theme valid: {theme_validation['value']}\")\n",
    "#     print(f\"Transitions valid: {transition_validation['is_valid']}\")\n",
    "    \n",
    "#     if not transition_validation['is_valid']:\n",
    "#         print(\"\\nInvalid transitions:\")\n",
    "#         for t in transition_validation['invalid_transitions']:\n",
    "#             print(f\"- {t['reason']}\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Error during graph generation or validation: {str(e)}\")\n",
    "#     raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmitriimartynov/Documents/Projects/chatsky-llm-autoconfig/.venv/lib/python3.12/site-packages/pydantic/_internal/_fields.py:172: UserWarning: Field name \"validate\" in \"Dialogue\" shadows an attribute in parent \"BaseModel\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Generating graph for topic: technical support conversation\n",
      "==================================================\n",
      "Generating Graph ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Checking graph requirements...\n",
      "üîÑ Found 30 cycles in the graph:\n",
      "Cycle 1: 1 -> 2 -> 5 -> 7 -> 11 -> 1\n",
      "Cycle 2: 1 -> 2 -> 5 -> 7 -> 12 -> 1\n",
      "Cycle 3: 1 -> 2 -> 5 -> 8 -> 11 -> 1\n",
      "Cycle 4: 1 -> 2 -> 5 -> 8 -> 12 -> 1\n",
      "Cycle 5: 1 -> 2 -> 6 -> 9 -> 11 -> 1\n",
      "Cycle 6: 1 -> 2 -> 6 -> 9 -> 12 -> 1\n",
      "Cycle 7: 1 -> 2 -> 6 -> 10 -> 11 -> 1\n",
      "Cycle 8: 1 -> 2 -> 6 -> 10 -> 12 -> 1\n",
      "Cycle 9: 1 -> 2 -> 1\n",
      "Cycle 10: 1 -> 3 -> 14 -> 16 -> 1\n",
      "Cycle 11: 1 -> 3 -> 14 -> 17 -> 11 -> 1\n",
      "Cycle 12: 1 -> 3 -> 14 -> 17 -> 16 -> 1\n",
      "Cycle 13: 1 -> 3 -> 15 -> 18 -> 1\n",
      "Cycle 14: 1 -> 3 -> 15 -> 19 -> 11 -> 1\n",
      "Cycle 15: 1 -> 3 -> 15 -> 19 -> 18 -> 1\n",
      "Cycle 16: 1 -> 3 -> 1\n",
      "Cycle 17: 1 -> 4 -> 20 -> 1\n",
      "Cycle 18: 1 -> 4 -> 1\n",
      "Cycle 19: 1 -> 4 -> 21 -> 20 -> 1\n",
      "Cycle 20: 1 -> 4 -> 21 -> 1\n",
      "Cycle 21: 3 -> 14 -> 17 -> 3\n",
      "Cycle 22: 3 -> 14 -> 3\n",
      "Cycle 23: 3 -> 15 -> 19 -> 3\n",
      "Cycle 24: 3 -> 15 -> 3\n",
      "Cycle 25: 2 -> 5 -> 7 -> 2\n",
      "Cycle 26: 2 -> 5 -> 8 -> 2\n",
      "Cycle 27: 2 -> 5 -> 2\n",
      "Cycle 28: 2 -> 6 -> 9 -> 2\n",
      "Cycle 29: 2 -> 6 -> 10 -> 2\n",
      "Cycle 30: 2 -> 6 -> 2\n",
      "‚úÖ Graph meets cycle requirements\n",
      "Sampling dialogues...\n"
     ]
    }
   ],
   "source": [
    "from chatsky_llm_autoconfig.algorithms.cycle_graph_generation_pipeline import GraphGenerationPipeline\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from chatsky_llm_autoconfig.schemas import GraphGenerationResult\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def generate_graphs():\n",
    "    output_dir = Path(\"generated_graphs\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    generation_model = ChatOpenAI(\n",
    "        model='o1-mini',\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "        temperature=1\n",
    "    )\n",
    "    \n",
    "    validation_model = ChatOpenAI(\n",
    "        model='gpt-4o',\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    pipeline = GraphGenerationPipeline(\n",
    "        generation_model=generation_model,\n",
    "        validation_model=validation_model\n",
    "    )\n",
    "    \n",
    "    topics = [\n",
    "        \"technical support conversation\",\n",
    "        # \"restaurant reservation\",\n",
    "        # \"online shopping checkout\",\n",
    "        # \"job interview\",\n",
    "        # \"travel booking\"\n",
    "    ]\n",
    "    \n",
    "    successful_generations = []\n",
    "    \n",
    "    for topic in topics:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Generating graph for topic: {topic}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        try:\n",
    "            result = pipeline(topic)\n",
    "            \n",
    "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
    "            if isinstance(result, GraphGenerationResult):\n",
    "                print(f\"‚úÖ Successfully generated graph for {topic}\")\n",
    "                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –≥—Ä–∞—Ñ–æ–º –∏ –¥–∏–∞–ª–æ–≥–∞–º–∏\n",
    "                successful_generations.append({\n",
    "                    \"graph\": result.graph,\n",
    "                    \"topic\": result.topic,\n",
    "                    \"dialogues\": result.dialogues\n",
    "                })\n",
    "            else:  # isinstance(result, GenerationError)\n",
    "                print(f\"‚ùå Failed to generate graph for {topic}\")\n",
    "                print(f\"Error type: {result.error_type}\")\n",
    "                print(f\"Error message: {result.message}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Unexpected error processing topic '{topic}': {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if successful_generations:\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"generated_graphs_{timestamp}.json\"\n",
    "        with open(output_dir / filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            # –ò—Å–ø–æ–ª—å–∑—É–µ–º model_dump() –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Pydantic –º–æ–¥–µ–ª–µ–π\n",
    "            json_data = [result for result in successful_generations]\n",
    "            json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"\\nAll graphs saved to: {filename}\")\n",
    "        \n",
    "        print(f\"\\nSuccessfully generated {len(successful_generations)} graphs out of {len(topics)} topics\")\n",
    "    else:\n",
    "        print(\"\\nNo graphs were successfully generated\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_graphs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
